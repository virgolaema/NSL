{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercitazione 11\n",
    "\n",
    "## Esercizio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definisco la funzione polinomiale da predire:\n",
    "$$\n",
    "f(x)=4-3x-2x^2+3x^3\n",
    "$$\n",
    "\n",
    "Nel generare i dati con rumore stocastico, pongo $\\sigma = 0.5$. Genero due serie di dati di validazione: la prima cade nell'intervallo usato per il training, ovvero $[-1,1]$ la seconda invece copre  l'intervallo successivo: $[1,3]$. Valuter√≤ poi se il modello ha un comportamento peggiore nel secondo caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def polynomial (x):\n",
    "    a = 4 \n",
    "    b = -3\n",
    "    c = -2 \n",
    "    d = 3\n",
    "    return a + b*x +c*x**2 + d*x**3\n",
    "\n",
    "nsamples = 100\n",
    "nsamples_valid = 30\n",
    "\n",
    "np.random.seed(278)\n",
    "x_train = np.random.uniform(-1, 1, nsamples)\n",
    "x_valid1 = np.random.uniform(-1, 1, nsamples_valid)\n",
    "x_valid2 = np.random.uniform(1, 2, nsamples_valid)\n",
    "x_valid1.sort()\n",
    "x_valid2.sort()\n",
    "y_target1 = polynomial(x_valid1)\n",
    "y_target2 = polynomial(x_valid2)\n",
    "\n",
    "sigma = 0.5 # noise standard deviation, \n",
    "y_train = np.random.normal(polynomial(x_train), sigma) \n",
    "y_valid1 = np.random.normal(polynomial(x_valid1), sigma)\n",
    "y_valid2 = np.random.normal(polynomial(x_valid2), sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training and target dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (14,6))\n",
    "plt.plot(x_valid1, y_target1)\n",
    "plt.scatter(x_valid1, y_valid1, color='r')\n",
    "plt.plot(x_valid2, y_target2)\n",
    "plt.scatter(x_valid2, y_valid2, color='g')\n",
    "plt.grid(True); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ottimizzazione dei parametri\n",
    "\n",
    "Creo ora la rete neurale. Vario i diversi parametri della rete neurale:\n",
    "- il numero di layers\n",
    "- il numero di neuroni per ogni layer\n",
    "- la funzione di attivazione\n",
    "- l'ottimizzatore\n",
    "- la funzione di costo\n",
    "\n",
    "Utilizzo il set di validazione per valutare le performance delle diverse reti, e salvo la migliore ottenuta da questa ampia grid search. Eseguo separamente l'analisi utilizzando come loss function prima un Mean Squared Error e poi un Mean Absolute Error, poich√® non posso confrontarli direttamente. \n",
    "\n",
    "Per ogni NN, genero dei valori casuali di x e rappresento le y predette nell'intervallo designato [-1,1] e in un intervallo successivo [1,2]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300  options explored\n",
      "------------------- Progression: 0.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=tanh opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3671 - mse: 0.3671\n",
      "------------------- Progression: 0.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=tanh opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2739 - mse: 0.2739\n",
      "------------------- Progression: 0.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=tanh opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3547 - mse: 0.3547\n",
      "------------------- Progression: 1.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=tanh opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3461 - mse: 0.3461\n",
      "------------------- Progression: 1.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=tanh opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2489 - mse: 0.2489\n",
      "------------------- Progression: 1.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=tanh opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2717 - mse: 0.2717\n",
      "------------------- Progression: 2.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=tanh opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.2588 - mse: 5.2588\n",
      "------------------- Progression: 2.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=tanh opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5395 - mse: 0.5395\n",
      "------------------- Progression: 2.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=tanh opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4427 - mse: 0.4427\n",
      "------------------- Progression: 3.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=tanh opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2969 - mse: 0.2969\n",
      "------------------- Progression: 3.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=tanh opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4210 - mse: 0.4210\n",
      "------------------- Progression: 3.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=tanh opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5986 - mse: 0.5986\n",
      "------------------- Progression: 4.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=relu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2990 - mse: 0.2990\n",
      "------------------- Progression: 4.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=relu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 4.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=relu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2668 - mse: 0.2668\n",
      "------------------- Progression: 5.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=relu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2954 - mse: 0.2954\n",
      "------------------- Progression: 5.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=relu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2448 - mse: 0.2448\n",
      "------------------- Progression: 5.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=relu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3434 - mse: 0.3434\n",
      "------------------- Progression: 6.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=relu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 6.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=relu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.4270 - mse: 1.4270\n",
      "------------------- Progression: 6.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=relu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5889 - mse: 0.5889\n",
      "------------------- Progression: 7.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=relu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3093 - mse: 0.3093\n",
      "------------------- Progression: 7.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=relu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 7.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=relu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3853 - mse: 0.3853\n",
      "------------------- Progression: 8.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=elu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3750 - mse: 0.3750\n",
      "------------------- Progression: 8.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=elu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3269 - mse: 0.3269\n",
      "------------------- Progression: 8.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=elu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4136 - mse: 0.4136\n",
      "------------------- Progression: 9.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=elu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4437 - mse: 0.4437\n",
      "------------------- Progression: 9.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=elu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2972 - mse: 0.2972\n",
      "------------------- Progression: 9.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=elu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2469 - mse: 0.2469\n",
      "------------------- Progression: 10.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=elu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.6878 - mse: 5.6878\n",
      "------------------- Progression: 10.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=elu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4453 - mse: 0.4453\n",
      "------------------- Progression: 10.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=elu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4259 - mse: 0.4259\n",
      "------------------- Progression: 11.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=elu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3917 - mse: 0.3917\n",
      "------------------- Progression: 11.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=elu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2775 - mse: 0.2775\n",
      "------------------- Progression: 11.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=elu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2627 - mse: 0.2627\n",
      "------------------- Progression: 12.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=sigmoid opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4429 - mse: 0.4429\n",
      "------------------- Progression: 12.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=sigmoid opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1770 - mse: 1.1770\n",
      "------------------- Progression: 12.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=sigmoid opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 13.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=sigmoid opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4490 - mse: 0.4490\n",
      "------------------- Progression: 13.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=sigmoid opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3010 - mse: 0.3010\n",
      "------------------- Progression: 13.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=sigmoid opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4268 - mse: 0.4268\n",
      "------------------- Progression: 14.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=sigmoid opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.6313 - mse: 2.6313\n",
      "------------------- Progression: 14.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=sigmoid opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.7756 - mse: 1.7756\n",
      "------------------- Progression: 14.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=sigmoid opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 15.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=sigmoid opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4338 - mse: 0.4338\n",
      "------------------- Progression: 15.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=sigmoid opt=RMSprop loss=mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 15.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=sigmoid opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 16.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=softmax opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0349 - mse: 1.0349\n",
      "------------------- Progression: 16.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=softmax opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 16.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=softmax opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2166 - mse: 1.2166\n",
      "------------------- Progression: 17.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=softmax opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.2444 - mse: 2.2444\n",
      "------------------- Progression: 17.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=softmax opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.1262 - mse: 3.1262\n",
      "------------------- Progression: 17.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=softmax opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.0664 - mse: 3.0664\n",
      "------------------- Progression: 18.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=softmax opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.1908 - mse: 10.1908\n",
      "------------------- Progression: 18.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=softmax opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 18.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=softmax opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 19.00 %\n",
      "This is: Nlayer=3 Nneur=30 act=softmax opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.5573 - mse: 2.5573\n",
      "------------------- Progression: 19.33 %\n",
      "This is: Nlayer=5 Nneur=30 act=softmax opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.8461 - mse: 2.8461\n",
      "------------------- Progression: 19.67 %\n",
      "This is: Nlayer=10 Nneur=30 act=softmax opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.8725 - mse: 2.8725\n",
      "------------------- Progression: 20.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=tanh opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3429 - mse: 0.3429\n",
      "------------------- Progression: 20.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=tanh opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3464 - mse: 0.3464\n",
      "------------------- Progression: 20.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=tanh opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.4366 - mse: 0.4366\n",
      "------------------- Progression: 21.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=tanh opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3155 - mse: 0.3155\n",
      "------------------- Progression: 21.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=tanh opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2668 - mse: 0.2668\n",
      "------------------- Progression: 21.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=tanh opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3153 - mse: 0.3153\n",
      "------------------- Progression: 22.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=tanh opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.9797 - mse: 1.9797\n",
      "------------------- Progression: 22.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=tanh opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4102 - mse: 0.4102\n",
      "------------------- Progression: 22.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=tanh opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4115 - mse: 0.4115\n",
      "------------------- Progression: 23.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=tanh opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3291 - mse: 0.3291\n",
      "------------------- Progression: 23.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=tanh opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3724 - mse: 0.3724\n",
      "------------------- Progression: 23.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=tanh opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3359 - mse: 0.3359\n",
      "------------------- Progression: 24.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=relu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 24.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=relu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3451 - mse: 0.3451\n",
      "------------------- Progression: 24.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=relu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2948 - mse: 0.2948\n",
      "------------------- Progression: 25.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=relu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3143 - mse: 0.3143\n",
      "------------------- Progression: 25.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=relu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2570 - mse: 0.2570\n",
      "------------------- Progression: 25.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=relu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 26.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=relu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5.7579 - mse: 5.7579\n",
      "------------------- Progression: 26.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=relu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 26.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=relu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3563 - mse: 0.3563\n",
      "------------------- Progression: 27.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=relu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2612 - mse: 0.2612\n",
      "------------------- Progression: 27.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=relu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 27.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=relu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5963 - mse: 0.5963\n",
      "------------------- Progression: 28.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=elu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3721 - mse: 0.3721\n",
      "------------------- Progression: 28.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=elu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3770 - mse: 0.3770\n",
      "------------------- Progression: 28.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=elu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3561 - mse: 0.3561\n",
      "------------------- Progression: 29.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=elu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4502 - mse: 0.4502\n",
      "------------------- Progression: 29.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=elu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2660 - mse: 0.2660\n",
      "------------------- Progression: 29.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=elu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2448 - mse: 0.2448\n",
      "------------------- Progression: 30.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=elu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.4231 - mse: 4.4231\n",
      "------------------- Progression: 30.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=elu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4221 - mse: 0.4221\n",
      "------------------- Progression: 30.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=elu opt=Adagrad loss=mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3646 - mse: 0.3646\n",
      "------------------- Progression: 31.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=elu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4420 - mse: 0.4420\n",
      "------------------- Progression: 31.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=elu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3180 - mse: 0.3180\n",
      "------------------- Progression: 31.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=elu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2956 - mse: 0.2956\n",
      "------------------- Progression: 32.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=sigmoid opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 32.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=sigmoid opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2323 - mse: 1.2323\n",
      "------------------- Progression: 32.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=sigmoid opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 33.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=sigmoid opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 33.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=sigmoid opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3132 - mse: 0.3132\n",
      "------------------- Progression: 33.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=sigmoid opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4326 - mse: 0.4326\n",
      "------------------- Progression: 34.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=sigmoid opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.2565 - mse: 2.2565\n",
      "------------------- Progression: 34.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=sigmoid opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 34.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=sigmoid opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 35.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=sigmoid opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3067 - mse: 0.3067\n",
      "------------------- Progression: 35.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=sigmoid opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3195 - mse: 0.3195\n",
      "------------------- Progression: 35.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=sigmoid opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 36.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=softmax opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.1934 - mse: 1.1934\n",
      "------------------- Progression: 36.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=softmax opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 36.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=softmax opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 37.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=softmax opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 37.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=softmax opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.2124 - mse: 3.2124\n",
      "------------------- Progression: 37.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=softmax opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 38.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=softmax opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10.2746 - mse: 10.2746\n",
      "------------------- Progression: 38.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=softmax opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 38.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=softmax opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 39.00 %\n",
      "This is: Nlayer=3 Nneur=50 act=softmax opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 39.33 %\n",
      "This is: Nlayer=5 Nneur=50 act=softmax opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 39.67 %\n",
      "This is: Nlayer=10 Nneur=50 act=softmax opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 40.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=tanh opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3779 - mse: 0.3779\n",
      "------------------- Progression: 40.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=tanh opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4497 - mse: 0.4497\n",
      "------------------- Progression: 40.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=tanh opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3674 - mse: 0.3674\n",
      "------------------- Progression: 41.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=tanh opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2978 - mse: 0.2978\n",
      "------------------- Progression: 41.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=tanh opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2839 - mse: 0.2839\n",
      "------------------- Progression: 41.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=tanh opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3786 - mse: 0.3786\n",
      "------------------- Progression: 42.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=tanh opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8456 - mse: 0.8456\n",
      "------------------- Progression: 42.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=tanh opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4398 - mse: 0.4398\n",
      "------------------- Progression: 42.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=tanh opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4090 - mse: 0.4090\n",
      "------------------- Progression: 43.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=tanh opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3222 - mse: 0.3222\n",
      "------------------- Progression: 43.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=tanh opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4488 - mse: 0.4488\n",
      "------------------- Progression: 43.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=tanh opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6144 - mse: 0.6144\n",
      "------------------- Progression: 44.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=relu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3044 - mse: 0.3044\n",
      "------------------- Progression: 44.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=relu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 44.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=relu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5675 - mse: 0.5675\n",
      "------------------- Progression: 45.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=relu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 45.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=relu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2801 - mse: 0.2801\n",
      "------------------- Progression: 45.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=relu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3266 - mse: 0.3266\n",
      "------------------- Progression: 46.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=relu opt=Adagrad loss=mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step - loss: 3.9754 - mse: 3.9754\n",
      "------------------- Progression: 46.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=relu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 46.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=relu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 47.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=relu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3106 - mse: 0.3106\n",
      "------------------- Progression: 47.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=relu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5063 - mse: 0.5063\n",
      "------------------- Progression: 47.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=relu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 48.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=elu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4082 - mse: 0.4082\n",
      "------------------- Progression: 48.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=elu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3063 - mse: 0.3063\n",
      "------------------- Progression: 48.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=elu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3650 - mse: 0.3650\n",
      "------------------- Progression: 49.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=elu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4154 - mse: 0.4154\n",
      "------------------- Progression: 49.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=elu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2984 - mse: 0.2984\n",
      "------------------- Progression: 49.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=elu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2777 - mse: 0.2777\n",
      "------------------- Progression: 50.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=elu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.4832 - mse: 2.4832\n",
      "------------------- Progression: 50.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=elu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4369 - mse: 0.4369\n",
      "------------------- Progression: 50.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=elu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3493 - mse: 0.3493\n",
      "------------------- Progression: 51.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=elu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4271 - mse: 0.4271\n",
      "------------------- Progression: 51.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=elu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4232 - mse: 0.4232\n",
      "------------------- Progression: 51.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=elu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4798 - mse: 0.4798\n",
      "------------------- Progression: 52.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=sigmoid opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 52.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=sigmoid opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.4046 - mse: 1.4046\n",
      "------------------- Progression: 52.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=sigmoid opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2386 - mse: 1.2386\n",
      "------------------- Progression: 53.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=sigmoid opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4410 - mse: 0.4410\n",
      "------------------- Progression: 53.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=sigmoid opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 53.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=sigmoid opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4863 - mse: 0.4863\n",
      "------------------- Progression: 54.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=sigmoid opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7519 - mse: 0.7519\n",
      "------------------- Progression: 54.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=sigmoid opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.2087 - mse: 1.2087\n",
      "------------------- Progression: 54.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=sigmoid opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2185 - mse: 1.2185\n",
      "------------------- Progression: 55.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=sigmoid opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 55.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=sigmoid opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 55.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=sigmoid opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4125 - mse: 0.4125\n",
      "------------------- Progression: 56.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=softmax opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 56.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=softmax opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2108 - mse: 1.2108\n",
      "------------------- Progression: 56.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=softmax opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 57.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=softmax opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.6663 - mse: 2.6663\n",
      "------------------- Progression: 57.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=softmax opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 57.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=softmax opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 58.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=softmax opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 10.4864 - mse: 10.4864\n",
      "------------------- Progression: 58.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=softmax opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 58.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=softmax opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 10.4124 - mse: 10.4124\n",
      "------------------- Progression: 59.00 %\n",
      "This is: Nlayer=3 Nneur=80 act=softmax opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 59.33 %\n",
      "This is: Nlayer=5 Nneur=80 act=softmax opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.1160 - mse: 3.1160\n",
      "------------------- Progression: 59.67 %\n",
      "This is: Nlayer=10 Nneur=80 act=softmax opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 60.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=tanh opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3657 - mse: 0.3657\n",
      "------------------- Progression: 60.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=tanh opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2891 - mse: 0.2891\n",
      "------------------- Progression: 60.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=tanh opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3160 - mse: 0.3160\n",
      "------------------- Progression: 61.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=tanh opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3020 - mse: 0.3020\n",
      "------------------- Progression: 61.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=tanh opt=Adam loss=mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3217 - mse: 0.3217\n",
      "------------------- Progression: 61.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=tanh opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2862 - mse: 0.2862\n",
      "------------------- Progression: 62.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=tanh opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8297 - mse: 0.8297\n",
      "------------------- Progression: 62.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=tanh opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4314 - mse: 0.4314\n",
      "------------------- Progression: 62.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=tanh opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4641 - mse: 0.4641\n",
      "------------------- Progression: 63.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=tanh opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3022 - mse: 0.3022\n",
      "------------------- Progression: 63.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=tanh opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4537 - mse: 0.4537\n",
      "------------------- Progression: 63.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=tanh opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3417 - mse: 0.3417\n",
      "------------------- Progression: 64.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=relu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 64.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=relu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3372 - mse: 0.3372\n",
      "------------------- Progression: 64.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=relu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3146 - mse: 0.3146\n",
      "------------------- Progression: 65.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=relu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2413 - mse: 0.2413\n",
      "------------------- Progression: 65.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=relu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3247 - mse: 0.3247\n",
      "------------------- Progression: 65.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=relu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3153 - mse: 0.3153\n",
      "------------------- Progression: 66.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=relu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 66.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=relu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 66.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=relu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2852 - mse: 0.2852\n",
      "------------------- Progression: 67.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=relu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2916 - mse: 0.2916\n",
      "------------------- Progression: 67.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=relu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 67.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=relu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3569 - mse: 0.3569\n",
      "------------------- Progression: 68.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=elu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4875 - mse: 0.4875\n",
      "------------------- Progression: 68.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=elu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3515 - mse: 0.3515\n",
      "------------------- Progression: 68.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=elu opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.2987 - mse: 0.2987\n",
      "------------------- Progression: 69.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=elu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4081 - mse: 0.4081\n",
      "------------------- Progression: 69.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=elu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2692 - mse: 0.2692\n",
      "------------------- Progression: 69.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=elu opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3415 - mse: 0.3415\n",
      "------------------- Progression: 70.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=elu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5489 - mse: 0.5489\n",
      "------------------- Progression: 70.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=elu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4489 - mse: 0.4489\n",
      "------------------- Progression: 70.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=elu opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3543 - mse: 0.3543\n",
      "------------------- Progression: 71.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=elu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4113 - mse: 0.4113\n",
      "------------------- Progression: 71.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=elu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6893 - mse: 0.6893\n",
      "------------------- Progression: 71.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=elu opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8727 - mse: 0.8727\n",
      "------------------- Progression: 72.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=sigmoid opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4630 - mse: 0.4630\n",
      "------------------- Progression: 72.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=sigmoid opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 72.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=sigmoid opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 73.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=sigmoid opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4699 - mse: 0.4699\n",
      "------------------- Progression: 73.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=sigmoid opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3252 - mse: 0.3252\n",
      "------------------- Progression: 73.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=sigmoid opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4476 - mse: 0.4476\n",
      "------------------- Progression: 74.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=sigmoid opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.4216 - mse: 1.4216\n",
      "------------------- Progression: 74.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=sigmoid opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 74.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=sigmoid opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2120 - mse: 1.2120\n",
      "------------------- Progression: 75.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=sigmoid opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2879 - mse: 0.2879\n",
      "------------------- Progression: 75.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=sigmoid opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 75.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=sigmoid opt=RMSprop loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3519 - mse: 0.3519\n",
      "------------------- Progression: 76.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=softmax opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2141 - mse: 1.2141\n",
      "------------------- Progression: 76.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=softmax opt=sgd loss=mse\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2093 - mse: 1.2093\n",
      "------------------- Progression: 76.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=softmax opt=sgd loss=mse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 77.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=softmax opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 77.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=softmax opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 77.67 %\n",
      "This is: Nlayer=10 Nneur=100 act=softmax opt=Adam loss=mse\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.4986 - mse: 3.4986\n",
      "------------------- Progression: 78.00 %\n",
      "This is: Nlayer=3 Nneur=100 act=softmax opt=Adagrad loss=mse\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.2051 - mse: 11.2051\n",
      "------------------- Progression: 78.33 %\n",
      "This is: Nlayer=5 Nneur=100 act=softmax opt=Adagrad loss=mse\n"
     ]
    }
   ],
   "source": [
    "# LOSS = MSE\n",
    "# parameters to vary\n",
    "layers = [3,5,10]\n",
    "neurons = [30,50,80,100,200]\n",
    "activation = ['tanh', 'relu', 'elu', 'sigmoid', 'softmax'] \n",
    "optimizers = ['sgd', 'Adam', 'Adagrad', 'RMSprop']\n",
    "lossfuncs = ['mse']\n",
    "nepochs = 200 #using earlystop, so not a problem\n",
    "\n",
    "nplots = len(layers)*len(neurons)*len(activation)*len(optimizers)*len(lossfuncs)\n",
    "print (nplots, ' options explored')\n",
    "\n",
    "# prepare plots\n",
    "ncols = 3\n",
    "nrows = int (nplots/ncols)\n",
    "fig_fit, axs = plt.subplots(nrows, ncols, figsize=(15, 3*nrows), tight_layout=True)\n",
    "\n",
    "i = 0 #just and index for the subplots\n",
    "best_score = 1e5 #store info for the best fit\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                  min_delta=0, patience=30, verbose=0, mode='auto')\n",
    "\n",
    "for my_loss in lossfuncs:\n",
    "    for n_neurons in neurons:\n",
    "        for my_activation in activation:\n",
    "            for my_optimizer in optimizers:\n",
    "                for nlayers in (layers):\n",
    "                    title = 'Nlayer='+ str(nlayers) +' Nneur='+ str(n_neurons)+' act='+my_activation+' opt='+my_optimizer+' loss='+my_loss\n",
    "                    print ('------------------- Progression: %.2f' %(i/nplots*100.), '%')\n",
    "                    print ('This is:',title)\n",
    "                    model = tf.keras.Sequential()\n",
    "                    for j in range (nlayers-1):\n",
    "                        if (j==0):\n",
    "                            model.add(Dense(n_neurons, activation = \"linear\", input_shape=(1,))) \n",
    "                        else:\n",
    "                            model.add(Dense(n_neurons, activation = my_activation))\n",
    "                    model.add(Dense(1, activation=\"relu\" ))  #final layer\n",
    "                    model.compile(optimizer=my_optimizer, loss=my_loss, metrics=['mse'])\n",
    "                    #model.summary()\n",
    "\n",
    "                    history = model.fit(x=x_train, y=y_train, \n",
    "                              batch_size=32, epochs=nepochs,\n",
    "                              shuffle=True, # a good idea is to shuffle input before each epoch\n",
    "                              validation_data=(x_valid1, y_valid1),                      \n",
    "                              callbacks = [earlystop],\n",
    "                              verbose = 0)\n",
    "                    model.get_weights()\n",
    "                    score = model.evaluate(x_valid1, y_valid1, batch_size=32, verbose=1)                    \n",
    "                    my_score = score[0]\n",
    "\n",
    "                    x_predicted = np.random.uniform(-1, 2, nsamples)\n",
    "                    y_predicted = model.predict(x_predicted)\n",
    "                    \n",
    "                    column = int(i/(nrows))\n",
    "                    row = int(i%(nrows))\n",
    "                    axs[row, column].set_title(title)\n",
    "                    axs[row, column].scatter(x_predicted, y_predicted,color='r', label ='prediction')\n",
    "                    axs[row, column].plot(x_valid1, y_target1, label = 'training interval')\n",
    "                    axs[row, column].plot(x_valid2, y_target2, label = 'other interval')\n",
    "                    axs[row, column].grid(True)\n",
    "                    \n",
    "                    if (my_score < best_score):\n",
    "                        best_score = my_score\n",
    "                        best_layers = nlayers \n",
    "                        best_neurons = n_neurons \n",
    "                        best_activation = my_activation \n",
    "                        best_optimizer = my_optimizer\n",
    "                        best_loss = my_loss\n",
    "                        best_title = title\n",
    "                        best_x = x_predicted\n",
    "                        best_y = y_predicted\n",
    "    \n",
    "                    del (model, y_predicted, history)\n",
    "                    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The best score achieved is', best_score, ' reached when: ', best_title)\n",
    "print ('Plotting best result using MSE as loss function')\n",
    "plt.figure(figsize=[14,6])\n",
    "plt.title(best_title)\n",
    "plt.scatter(best_x, best_y,color='r'label ='prediction')\n",
    "plt.plot(x_valid1, y_target1, label ='training interval')\n",
    "plt.plot(x_valid2, y_target2, label = 'other interval')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS = Mean Abs Error\n",
    "# parameters to vary\n",
    "layers = [3,5,10]\n",
    "neurons = [30,50,80,100,200]\n",
    "activation = ['tanh', 'relu', 'elu', 'sigmoid', 'softmax'] \n",
    "optimizers = ['sgd', 'Adam', 'Adagrad', 'RMSprop']\n",
    "lossfuncs = ['mean_absolute_error']\n",
    "nepochs = 200 #using earlystop, so not a problem\n",
    "\n",
    "nplots = len(layers)*len(neurons)*len(activation)*len(optimizers)*len(lossfuncs)\n",
    "print (nplots, ' options explored')\n",
    "\n",
    "# prepare plots\n",
    "ncols = 3\n",
    "nrows = int (nplots/ncols)\n",
    "fig_fit, axs = plt.subplots(nrows, ncols, figsize=(15, 3*nrows), tight_layout=True)\n",
    "\n",
    "i = 0 #just and index for the subplots\n",
    "best_score = 1e5 #store info for the best fit\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                  min_delta=0, patience=30, verbose=0, mode='auto')\n",
    "\n",
    "for my_loss in lossfuncs:\n",
    "    for n_neurons in neurons:\n",
    "        for my_activation in activation:\n",
    "            for my_optimizer in optimizers:\n",
    "                for nlayers in (layers):\n",
    "                    title = 'Nlayer='+ str(nlayers) +' Nneur='+ str(n_neurons)+' act='+my_activation+' opt='+my_optimizer+' loss='+my_loss\n",
    "                    print ('------------------- Progression: %.2f' %(i/nplots*100.), '%')\n",
    "                    print ('This is:',title)\n",
    "                    model = tf.keras.Sequential()\n",
    "                    for j in range (nlayers-1):\n",
    "                        if (j==0):\n",
    "                            model.add(Dense(n_neurons, activation = \"linear\", input_shape=(1,))) \n",
    "                        else:\n",
    "                            model.add(Dense(n_neurons, activation = my_activation))\n",
    "                    model.add(Dense(1, activation=\"relu\" ))  #final layer\n",
    "                    model.compile(optimizer=my_optimizer, loss=my_loss, metrics=['mse'])\n",
    "                    #model.summary()\n",
    "\n",
    "                    history = model.fit(x=x_train, y=y_train, \n",
    "                              batch_size=32, epochs=nepochs,\n",
    "                              shuffle=True, # a good idea is to shuffle input before each epoch\n",
    "                              validation_data=(x_valid1, y_valid1),                      \n",
    "                              callbacks = [earlystop],\n",
    "                              verbose = 0)\n",
    "                    model.get_weights()\n",
    "                    score = model.evaluate(x_valid1, y_valid1, batch_size=32, verbose=1)                    \n",
    "                    my_score = score[0]\n",
    "\n",
    "                    x_predicted = np.random.uniform(-1, 2, nsamples)\n",
    "                    y_predicted = model.predict(x_predicted)\n",
    "                    \n",
    "                    column = int(i/(nrows))\n",
    "                    row = int(i%(nrows))\n",
    "                    axs[row, column].set_title(title)\n",
    "                    axs[row, column].scatter(x_predicted, y_predicted,color='r', label ='prediction')\n",
    "                    axs[row, column].plot(x_valid1, y_target1, label = 'training interval')\n",
    "                    axs[row, column].plot(x_valid2, y_target2, label = 'other interval')\n",
    "                    axs[row, column].grid(True)\n",
    "                    \n",
    "                    if (my_score < best_score):\n",
    "                        best_score = my_score\n",
    "                        best_layers = nlayers \n",
    "                        best_neurons = n_neurons \n",
    "                        best_activation = my_activation \n",
    "                        best_optimizer = my_optimizer\n",
    "                        best_loss = my_loss\n",
    "                        best_title = title\n",
    "                        best_x = x_predicted\n",
    "                        best_y = y_predicted\n",
    "    \n",
    "                    del (model, y_predicted, history)\n",
    "                    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The best score achieved is', best_score, ' reached when: ', best_title)\n",
    "print ('Plotting best result using Mean Abs Error as loss function')\n",
    "plt.title(best_title)\n",
    "plt.scatter(best_x, best_y,color='r'label ='prediction')\n",
    "plt.plot(x_valid1, y_target1, label ='training interval')\n",
    "plt.plot(x_valid2, y_target2, label = 'other interval')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In generale, osservo che riesco ad ottenere una buona stima nel range utilizzato per il training, ma non nel range esterno a tale intervallo. \n",
    "\n",
    "Noto inoltre che aumentare molto il numero di layer e di neuroni non garantisce di costruire un modello che sappia predire perfettamente i dati, soprattutto perch√® si adatta eccessivamente ai dati di training e non risulta adatto ai dati di validazione. \n",
    "Questo accade perch√®, all'aumentare della complessit√† del modello, anche se si riduce l'errore di bias e migliora l'adattamento al training set, aumenta la varianza quando ci si trova davanti a nuovi dati. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.3\n",
    "  \n",
    "Try to extend the model to fit a simple trigonometric 2D function such as $f(x,y) = \\sin(x^2+y^2)$ in the range $x \\in [-3/2,3/2]$ and $y \\in [-3/2,3/2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to meditate on these exercises and judge your results can be found <a href=https://xkcd.com/2048/>here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Supplementary material: Keras model.fit available callbacks</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .fit method can also get <a href=https://keras.io/callbacks/>callback</a> functions which can be used to customize the fitting procedure with special actions.\n",
    "\n",
    "Keras provides some predefined callbacks to feed in, among them for example:\n",
    "- **TerminateOnNaN()**: that terminates training when a NaN loss is encountered\n",
    "- **ModelCheckpoint(filepath)**: that save the model after every epoch\n",
    "- **EarlyStopping()**: which stop training when a monitored quantity has stopped improving\n",
    "\n",
    "You can select one or more callback and pass them as a list to the callback argument of the fit method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to construct a callback object to represent how estimated parameters are converging during the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "class PlotCurrentEstimate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_valid, y_valid):\n",
    "        \"\"\"Keras Callback which plot current model estimate against reference target\"\"\"\n",
    "        \n",
    "        # convert numpy arrays into lists for plotting purposes\n",
    "        self.x_valid = list(x_valid[:])\n",
    "        self.y_valid = list(y_valid[:])\n",
    "        self.iter=0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        temp = self.model.predict(self.x_valid, batch_size=None, verbose=False, steps=None)\n",
    "        self.y_curr = list(temp[:]) # convert numpy array into list\n",
    "        \n",
    "        self.iter+=1\n",
    "        if self.iter%10 == 0:\n",
    "            clear_output(wait=True)            \n",
    "            self.eplot = plt.subplot(1,1,1)\n",
    "            self.eplot.clear()     \n",
    "            self.eplot.scatter(self.x_valid, self.y_curr, color=\"blue\", s=4, marker=\"o\", label=\"estimate\")\n",
    "            self.eplot.scatter(self.x_valid, self.y_valid, color=\"red\", s=4, marker=\"x\", label=\"valid\")\n",
    "            self.eplot.legend()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use also an EarlyStopping callback on the val_loss quantity. This will stop the training process as soon as the val_loss quantity does not improve anymore after an amount of epochs, preventing a long time of wated computation to take over without useful results.\n",
    "\n",
    "<code>keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)</code>\n",
    "\n",
    "Arguments:\n",
    "\n",
    "- <code>monitor</code>: quantity to be monitored. \n",
    "- <code>min_delta:</code> minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. \n",
    "- <code>patience:</code> number of epochs with no improvement after which training will be stopped. \n",
    "- <code>verbose:</code> verbosity mode. \n",
    "- <code>mode:</code> one of {auto, min, max}. In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. \n",
    "- <code>baseline:</code> Baseline value for the monitored quantity to reach. Training will stop if the model doesn't show improvement over the baseline. \n",
    "- <code>restore_best_weights:</code> whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[array([[1.8340337]], dtype=float32), array([2.0234365], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_estimate = PlotCurrentEstimate(x_valid, y_valid)\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                  min_delta=0, patience=100, mode='auto')\n",
    "\n",
    "model.fit(x_valid, y_valid, batch_size=32, epochs=150,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=[ plot_estimate, earlystop]\n",
    "          )\n",
    "\n",
    "model.get_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
